## Data-Science-Switching-Behaviour-in-Statutory-Health-Insurance

**Explaining the Explained: Leveraging LLMs to Interpret Switching Predictions in Health Insurance**

Status: Research in Progress
Focus: Machine Learning (ML) · Explainable AI (XAI) · Large Language Models (LLMs) · Health Insurance

**Project Motivation**

- Health insurers face increasing churn in competitive markets.

- ML is used to predict customer switching, but explanations are often too technical for stakeholders.

- There’s a need for clear, human-understandable insights—especially in sensitive domains like healthcare.

- We explore how Large Language Models (LLMs) can bridge this gap by translating complex XAI outputs into accessible narratives.

**Project Goals**

- Augment existing XAI tools (e.g., SHAP) with LLMs to improve interpretability of churn predictions.

- Identify the most influential features that drive customer switching.

- Automate evaluation of LLM-generated explanations—without relying on gold-standard references.

- Develop methods that support transparent and user-friendly AI in the health insurance domain.

**Key Research Questions**

- **RQ1:** Which customer satisfaction dimensions most strongly predict switching intent?

- **RQ2:** How do SHAP explanations compare to those generated by LLMs?

- **RQ3:** Does fine-tuning LLMs on structured features and SHAP outputs improve explanation quality vs. zero-shot prompts?

- **RQ4:** How can we automatically evaluate the quality of LLM explanations when no ground truth exists?

**Why This Matters**

- Traditional ML models lack transparency—critical for trust in healthcare and insurance.

- Current XAI tools (e.g., SHAP, LIME) still require technical literacy.

- LLMs can generate natural language explanations, enhancing usability and stakeholder trust.

- Our proposed evaluation method enables scalable testing of LLM explanations using:

      Keyword and synonym matching

      Cosine similarity analysis

**State of the Art Insights**

- Churn prediction models (LightGBM, decision trees, etc.) are effective but hard to interpret.

- SHAP provides local feature attribution but isn't easily understood by non-experts.

- LLMs can translate complex insights into narratives, but quality and reliability remain open questions.

--> We aim to close these gaps through LLM integration, fine-tuning, and automated evaluation frameworks.

**Expected Impact**

- Enable more transparent and interpretable ML in health insurance churn prediction.

- Support data-driven decision-making with accessible AI explanations.

- Advance research in user-centric XAI and LLM evaluation.
